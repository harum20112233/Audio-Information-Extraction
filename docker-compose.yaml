# ============================================
# docker-compose.yaml
# 使い方:
#   - 同ディレクトリに .env を置くと HUGGINGFACE_TOKEN が自動で読み込まれます
#   - 起動:    docker compose up --build
#   - 一回実行: docker compose run --rm app python -m src.pipeline --in samples/sample.wav --out data/out/result.csv
#   - 終了:    Ctrl+C（常駐中） or 別ターミナルで `docker compose down`
# 前提:
#   - WSL2 + Ubuntu + NVIDIA Container Toolkit がセットアップ済み（`--gpus all` が使える）
# ============================================

# 複数コンテナ（サービス）の集合を定義するトップレベルキー
services:
  # サービス名。自由に変更可（例: app, backend など）
  app:
    # ビルド方法。カレントディレクトリの Dockerfile でイメージをビルドする指定
    build: .

    # コンテナにGPUを割り当てる指定。NVIDIA Container Toolkit 導入済みが前提
    gpus: all

    # コンテナ内の作業ディレクトリ。以後の相対パスはここを基準に解決される
    working_dir: /work

    # ホスト⇔コンテナのディレクトリ共有（ボリューム）を列挙
    volumes:
      # プロジェクト一式をコンテナの /work にマウント（編集が即反映される）
      - ./:/work

      # 入出力データ（CSVや生成物）を永続化するためのマウント
      - ./data:/work/data

      # Hugging Face のキャッシュをホスト側に保存して再ダウンロードを避ける
      - ./cache/hf:/root/.cache/huggingface

      # Transformers 系のキャッシュ（モデルやトークナイザ）をホスト側に保存
      - ./cache/transformers:/root/.cache/torch/transformers

      - ./cache/whisper:/root/.cache/whisper # openai-whisperのキャッシュを永続化

      # ログ出力をホスト側に保存（学習/推論のログ確認が容易）
      - ./logs:/work/logs

      # サンプル音声を共有（テスト実行用）
      - ./samples:/work/samples

    # コンテナに渡す環境変数を定義（.env から補完される）
    environment:
      # Python の出力をバッファリングしない（リアルタイムでログに出力）
      - PYTHONUNBUFFERED=1
      # Hugging Face のアクセストークン。必要なモデルで認証が要る場合に使用
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
      - HF_TOKEN=${HUGGINGFACE_TOKEN}

      # pyannote の利用可否
      - USE_PYANNOTE=${USE_PYANNOTE}

      # HF のキャッシュディレクトリを明示（上のボリュームとパスを一致させる）
      - HF_HOME=/root/.cache/huggingface

      # Transformers のキャッシュディレクトリを明示（上のボリュームとパスを一致させる）
      - TRANSFORMERS_CACHE=/root/.cache/torch/transformers

    # 疑似端末を割り当てる。printの改行や対話実行が安定するため開発時は true 推奨
    tty: true

    #デバッグ専用プロファイル
  app-debug:
    extends:
      service: app
    profiles:
      - debug
    ports:
      - "5678:5678"
    command: >
      python -m debugpy --wait-for-client --listen 0.0.0.0:5678
      -m src.pipeline
      --in samples/amagasaki/amagasaki__2014_10_28_32s.mp3
      --out data/out/result.csv
      --whisper_model small
      --language ja
      --device auto
      --sentiment_model llm-book/bert-base-japanese-v3-wrime-sentiment

  # 学習デバッグ専用プロファイル
  train-debug:
    extends:
      service: app
    profiles:
      - debug
    ports:
      - "5679:5679"
    command: >
      python -m debugpy --wait-for-client --listen 0.0.0.0:5679
      -m src.train_asr
      --base_model openai/whisper-small
      --train_csv train_data/asr_2min/train.csv
      --valid_csv train_data/asr_2min/valid.csv
      --output_dir models/whisper-small-ja-lora
      --use_lora
      --lora_r 16 --lora_alpha 32 --lora_dropout 0.05
      --num_train_epochs 5
      --per_device_train_batch_size 8
      --per_device_eval_batch_size 8
      --gradient_accumulation_steps 2
      --learning_rate 1e-5
      --weight_decay 0.01
      --warmup_ratio 0.1
      --fp16
      --gradient_checkpointing
      --merge_lora_and_save
