# 統合型音声分析システム(話者分離+感情推定) 要件定義書

---

## 1. 概要

### 1.1. 目的

本システムは、複数話者が存在する音声データに対し、**話者分離**（誰が）、**文字起こし**（何を）、**感情分析**（どのように）を自動で行うことを目的とする。
音声コンテンツから網羅的かつ構造化された情報を抽出し、議事録作成の自動化、コミュニケーション分析、コンテンツのデータ化など、多岐にわたる活用を実現する。

### 1.2. ゴール

- **入力**: 複数話者を含む単一の音声ファイル
- **処理**: 話者ごとの発話区間を特定し、各区間の音声に対して文字起こしと感情分析を並行して実行
- **出力**: 話者、発話時間、テキスト化された発話内容、感情分析結果を統合した構造化データ（`CSV`形式）を出力

---

## 2. 機能要件

### 2.1. 音声入力

- **対応フォーマット**: `WAV`形式（必須）、`MP3`形式（望ましい）
- **入力インターフェース**: ローカルファイルシステム上の音声ファイルを指定して処理を実行

### 2.2. 話者分離機能

- **コア技術**: `pyannote.audio` ライブラリの事前学習済みモデルを利用
- **処理内容**: 音声データから話者数を自動推定し、各話者の発話区間（開始時刻・終了時刻）と一意の話者 ID（例: `SPEAKER_00`）を特定

### 2.3. 感情分析機能

- **コア技術**: Hugging Face `transformers` ライブラリと事前学習済み音声感情認識モデルを利用
- **分析単位**: 話者分離で特定された個々の発話セグメント
- **分類カテゴリ（最低保証）**:
  - 喜び (Joy)
  - 怒り (Anger)
  - 悲しみ (Sadness)
  - 平常 (Neutral)
- **拡張可能カテゴリ**: fear, surprise など、モデルが対応している場合は追加分類可能
- **出力**: 感情ラベルと信頼度スコア

### 2.4. 文字起こし機能

- **コア技術**: OpenAI `Whisper` モデルを利用
- **処理単位**: 話者分離で特定された個々の発話セグメント
- **出力**: 各発話セグメントに対応するテキスト

### 2.5. 結果出力

- **必須フォーマット**: `CSV`
- **望ましいフォーマット**: `JSON`（オプション機能として拡張可能）
- **データ項目**:
  - `start_time` (発話開始時刻)
  - `end_time` (発話終了時刻)
  - `speaker_id` (話者 ID)
  - `text` (文字起こしされた発話内容)
  - `emotion` (感情ラベル)
  - `score` (感情分析の信頼度スコア)

---

## 3. 非機能要件

### 3.1. 可搬性と再現性

- **実行環境**: Docker コンテナとしてパッケージング
- **依存性解決**: Python バージョン、システムライブラリ、各種モデルを含め Docker イメージ内にカプセル化
- **セットアップ**: Docker があれば数コマンドでビルド・実行可能

### 3.2. 性能

- **ハードウェア要件**: NVIDIA GPU (CUDA 対応) を推奨
- **処理速度**: GPU を前提に実用的速度を担保。CPU のみでも動作は可能だが低速

### 3.3. 精度

- **基準**: 各事前学習モデルの公称性能に準拠
- **制約**: ノイズ・重なり音声・専門用語・早口では精度が低下する可能性あり

### 3.4. セキュリティ

- **認証情報**: Hugging Face アクセストークン等は環境変数`.env`経由で渡す
- **データ保護**: 音声データはローカル処理。外部 API に送信しない設計を基本とする

### 3.5. ログ・エラーハンドリング

- **ログ出力**: 標準出力に INFO レベルの進捗ログを表示。`--verbose` で DEBUG ログを有効化可能
- **エラー処理**: 区間処理が失敗しても全体処理は継続。CSV では該当セルを空欄にする

---

## 4. 実行環境・技術スタック

- **実行基盤**: Docker Engine (Linux/macOS/Windows)
- **ベースイメージ**: `pytorch/pytorch`（CUDA サポートあり）
- **言語**: Python 3.9 以降
- **主要ライブラリ/モデル**:
  - `pyannote.audio`: 話者分離
  - `openai-whisper`: 文字起こし
  - `transformers`: 感情分析
  - `torch` / `torchaudio`: 機械学習基盤
  - `pydub`: 音声処理
  - `pandas`: データ出力

---

## 5. 成果物

- ソースコード一式 (Python スクリプト)
- `Dockerfile`
- `docker-compose.yml`
- `requirements.txt`
- `README.md`（セットアップ・実行方法の説明）
- **サンプル音声ファイル（短尺・ライセンスクリア済み）**
- **サンプル出力 CSV**

---

```
audio-ie/
  ├─ src/
  │   └─ pipeline.py
  ├─ requirements.txt
  ├─ Dockerfile
  └─ samples/
      └─ demo.wav   ← 短尺テスト音源（自前で置く）

```
